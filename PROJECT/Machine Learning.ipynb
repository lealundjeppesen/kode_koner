{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1f1c32-fe79-4a76-b4a9-aa0ab8d561c2",
   "metadata": {},
   "source": [
    "## Machine Learning: Building a regression model using Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aecdac56-b75e-481d-a3b3-cd7204f6ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>volume</th>\n",
       "      <th>daily_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.018395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.022147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.027972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.018487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.028071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.179449</td>\n",
       "      <td>12</td>\n",
       "      <td>0.172622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>1.444444</td>\n",
       "      <td>2.962731</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.012815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>-0.400000</td>\n",
       "      <td>1.776388</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.012189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.005449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2822 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean       std  volume  daily_return\n",
       "0    -0.500000  0.707107       2     -0.018395\n",
       "1     0.000000       NaN       1     -0.022147\n",
       "2     0.000000  0.000000       3      0.027972\n",
       "3     0.000000  0.000000       4      0.015306\n",
       "4    -0.333333  0.577350       3     -0.018487\n",
       "...        ...       ...     ...           ...\n",
       "2817  0.000000  0.000000       3      0.028071\n",
       "2818  0.250000  2.179449      12      0.172622\n",
       "2819  1.444444  2.962731       9     -0.012815\n",
       "2820 -0.400000  1.776388      10     -0.012189\n",
       "2821 -2.000000  2.828427       2     -0.005449\n",
       "\n",
       "[2822 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## IMPORTING THE CLEANED DATAFRAME\n",
    "df_final = pd.read_csv('./CSV/df_merged_clean.csv')\n",
    "#Print to ensure it is the right data frame\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972db6f6-5205-4001-ab7c-ec29a840b13d",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0970aff9-5f83-470d-bf17-077dafedbd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Transforming the data set and creating interaction terms\u001b[39;00m\n\u001b[1;32m      9\u001b[0m poly_trans \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, include_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m X_train_p \u001b[38;5;241m=\u001b[39m poly_trans\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m     11\u001b[0m X_test_p \u001b[38;5;241m=\u001b[39m poly_trans\u001b[38;5;241m.\u001b[39mtransform(X_test)  \u001b[38;5;66;03m# Use transform, not fit_transform for the test set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Rescaling our data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_polynomial.py:252\u001b[0m, in \u001b[0;36mPolynomialFeatures.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03mCompute number of output features.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Fitted transformer.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 252\u001b[0m _, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, Integral):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_bias:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Splitting the data into features and target variable\n",
    "X = df_final[['mean', 'std', 'volume']]  # Features\n",
    "y = df_final['daily_return']  # Target variable\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Transforming the data set and creating interaction terms\n",
    "poly_trans = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_p = poly_trans.fit_transform(X_train)\n",
    "X_test_p = poly_trans.transform(X_test)  # Use transform, not fit_transform for the test set\n",
    "\n",
    "# Rescaling our data\n",
    "rescaler = StandardScaler().fit(X_train_p)\n",
    "X_train2 = rescaler.transform(X_train_p)\n",
    "X_test2 = rescaler.transform(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98a689-9a46-48a9-a4f9-218bd4e15544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that calculates the mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "output = []\n",
    "lambdas = np.logspace(-4, 4, 20)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    reg = Lasso(alpha=lambda_, random_state=1)\n",
    "    reg.fit(X_train2, y_train)\n",
    "    output.append(\n",
    "        [\n",
    "            lambda_,\n",
    "            rmse(reg.predict(X_train2), y_train),\n",
    "            rmse(reg.predict(X_test2), y_test),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e826b1e-44a6-4d9b-aa4b-bb71654c502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and selection of the optimal lambda cvalue\n",
    "MSE_df = pd.DataFrame(\n",
    "    data=output, columns=[\"lambda\", \"MSE train\", \"MSE test\"]\n",
    ").set_index(\"lambda\")\n",
    "\n",
    "MSE_df.plot(logx=True, logy=True)\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for different lambda values')\n",
    "plt.show()\n",
    "\n",
    "best_fit = MSE_df[\"MSE test\"].nsmallest(1)\n",
    "lambda_opt, RMSE_min = next(best_fit.items())\n",
    "print(f\"Minimum RMSE = {RMSE_min:.3f} found for lambda = {lambda_opt:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcec91e-6fbe-4ff4-bc68-5de6ba05f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fitting the Lasso model\n",
    "reg = Lasso(alpha=lambda_opt)\n",
    "reg.fit(X_train2, y_train)\n",
    "\n",
    "# Creating predicted values\n",
    "y_pred = reg.predict(X_test2)\n",
    "\n",
    "# Extracting coefficients and intercept\n",
    "coefficients = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"\\nCoefficients:\")\n",
    "for feature, coef in zip(poly_trans.get_feature_names_out(X.columns), coefficients):\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b317dc-9198-4b3f-a680-dea135951608",
   "metadata": {},
   "source": [
    "### Residuals Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37929dc-07ca-429a-b795-617bc51e1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plotting residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')  # Adding a horizontal line at y=0\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d09409-9801-4644-b901-9377d95e61db",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bb5d4-41ab-462c-9097-9dda858b066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the optimal lambda value\n",
    "reg = Lasso(alpha=lambda_opt)\n",
    "\n",
    "# Computing cross-validated RMSE scores\n",
    "scores = cross_val_score(reg, X_train2, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"Cross-validated RMSE scores:\", rmse_scores)\n",
    "print(\"Mean RMSE:\", rmse_scores.mean())\n",
    "print(\"Standard Deviation of RMSE:\", rmse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8b8d7-367c-412b-9fed-5b85673c3c5b",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "Our model, on average, has an error of about 0.0234 (23,4%). This is the typical difference between the observed values and the model's predictions.\n",
    "\n",
    "The range of RMSE values across the 5 folds goes from around 0.017 to 0.036. This range gives you an idea of the best and worst-case scenarios for your model's performance on unseen data.\n",
    "\n",
    "The standard deviation of the RMSE values across the folds is 0.00675. This indicates that there's some variability in how well the model performs on different subsets of the data. If this value were very high, it might be a cause for concern, as it would indicate that the model's performance is very inconsistent. In your case, while there's some variability, it's not excessively high.\n",
    "\n",
    "In conclusion, cross-validation provides a more robust estimate of a model's performance than a single train-test split. By looking at the range and standard deviation of the RMSE values, you can get a sense of the model's consistency and reliability. If you're comparing multiple models, you'd typically choose the one with the lowest mean RMSE, provided its performance is consistent across folds (i.e., a low standard deviation of RMSE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
